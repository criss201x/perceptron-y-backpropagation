from google.colab import drive
drive.mount('/content/drive')


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data_mlp.csv")


df.info()


df.describe()


X = df[['x1','x2','x3']].to_numpy().astype(float)
y = df[['d']].to_numpy().astype(float)      # shape (N,1)


# split 80/20
rng = np.random.default_rng(42)#  utilizando una permutación aleatoria para asegurar una división aleatoria y reproducible (gracias a la semilla seed=42)
idx = rng.permutation(len(X)); n_tr = int(0.8*len(X))
tr, va = idx[:n_tr], idx[n_tr:]
Xtr, Xva = X[tr], X[va]; ytr, yva = y[tr], y[va]


# escalar X con estadísticas del train
# dividir train/val como antes...
# centra los datos de entrenamiento y validación restando la media del conjunto de entrenamiento. Esto es una práctica común para normalizar los datos antes de alimentar una red neuronal.
muX = Xtr.mean(axis=0)
Xtr_c = Xtr - muX
Xva_c = Xva - muX


# -------- utilidades --------
def relu(z): return np.maximum(0,z)
def relu_d(z): return (z>0).astype(z.dtype)
def mse(yhat,y): return np.mean((yhat-y)**2)
def mae(yhat,y): return np.mean(np.abs(yhat-y))


# -------- inicialización He --------
#d_in, d_h, d_out = 3, 16, 1
#W1 = rng.normal(0, np.sqrt(2/d_in), (d_in,d_h)); b1 = np.zeros(d_h)
#W2 = rng.normal(0, np.sqrt(2/d_h), (d_h,d_out)); b2 = np.zeros(d_out)
#Inicialización
#se encarga de la inicialización de los pesos (W) y sesgos (b) para la red neuronal.
#Utiliza la inicialización de He, que es adecuada para capas con función de activación ReLU.
#Aquí se definen las dimensiones de las capas de entrada (d_in), las capas ocultas (d_h1, d_h2) y la capa de salida (d_out)
d_in, d_h1, d_h2, d_out = 3, 32, 16, 1
W1 = rng.normal(0, np.sqrt(2/d_in),  (d_in,d_h1));  b1 = np.zeros(d_h1)
W2 = rng.normal(0, np.sqrt(2/d_h1), (d_h1,d_h2));  b2 = np.zeros(d_h2)
W3 = rng.normal(0, np.sqrt(2/d_h2), (d_h2,d_out)); b3 = np.zeros(d_out)


# -------- entrenamiento --------
# Esta función implementa el proceso de entrenamiento de la red neuronal multicapa. Incluye:
def train_mlp(Xtr_c, ytr, Xva_c, yva, *,
              d_h1=32, d_h2=16, lr=3e-4, l2=1e-5,
              epochs=2000, batch=32, seed=42):
    rng = np.random.default_rng(seed)

    # init He (ReLU)
    d_in, d_out = Xtr_c.shape[1], 1
    W1 = rng.normal(0, np.sqrt(2/d_in),  (d_in, d_h1)); b1 = np.zeros(d_h1)
    W2 = rng.normal(0, np.sqrt(2/d_h1), (d_h1, d_h2)); b2 = np.zeros(d_h2)
    W3 = rng.normal(0, np.sqrt(2/d_h2), (d_h2, d_out)); b3 = np.zeros(d_out)

    N = len(Xtr_c)
    def relu(z): return np.maximum(0, z)
    def mse(yh,y): return np.mean((yh-y)**2)

    for ep in range(epochs):
        # mini-batch
        for s in range(0, N, batch):
            xb, yb = Xtr_c[s:s+batch], ytr[s:s+batch]

            z1 = xb @ W1 + b1; h1 = relu(z1)
            z2 = h1 @ W2 + b2; h2 = relu(z2)
            yhat = h2 @ W3 + b3

            dY  = (yhat - yb)/len(xb)
            gW3 = h2.T @ dY + (2*l2/N)*W3; gb3 = dY.sum(0)

            dh2 = dY @ W3.T
            dz2 = dh2 * (z2 > 0)
            gW2 = h1.T @ dz2 + (2*l2/N)*W2; gb2 = dz2.sum(0)

            dh1 = dz2 @ W2.T
            dz1 = dh1 * (z1 > 0)
            gW1 = xb.T @ dz1 + (2*l2/N)*W1; gb1 = dz1.sum(0)

            W3 -= lr*gW3; b3 -= lr*gb3
            W2 -= lr*gW2; b2 -= lr*gb2
            W1 -= lr*gW1; b1 -= lr*gb1

        # simple LR decay opcional
        if (ep+1) % 500 == 0:
            lr *= 0.5

    # métrica final
    def predict(Xc):#  para realizar predicciones y calcula el error cuadrático medio (MSE) en el conjunto de validación.
        return relu(relu(Xc@W1+b1)@W2+b2)@W3+b3

    yva_hat = predict(Xva_c)
    return mse(yva_hat, yva)


# -------- predicción --------
# para probar diferentes tasas de aprendizaje (lr) definidas en grid_lr. Entrena el modelo con cada tasa de aprendizaje y
# muestra el MSE en el conjunto de validación para cada una. Esto ayuda a encontrar una tasa de aprendizaje adecuada para el entrenamiento.
grid_lr = [1e-4, 3e-4, 5e-4, 1e-3]
for lr in grid_lr:
    m = train_mlp(Xtr_c, ytr, Xva_c, yva, lr=lr, l2=1e-5, d_h1=32, d_h2=16, epochs=2000)
    print(f"lr={lr:.0e} -> MSE_va={m:.6f}")


'''
 realiza una búsqueda similar, pero esta vez varía el parámetro de regularización L2 (l2) utilizando los valores definidos en grid_l2.
 Entrena el modelo con cada valor de l2 (manteniendo el mejor lr encontrado anteriormente) y muestra el MSE en el conjunto de validación.
 Esto ayuda a determinar si la regularización L2 mejora el rendimiento del modelo y cuál es un buen valor para ella.
'''
grid_l2 = [0.0, 1e-6, 1e-5, 1e-4]
for l2 in grid_l2:
    m = train_mlp(Xtr_c, ytr, Xva_c, yva, lr=3e-4, l2=l2, d_h1=32, d_h2=16, epochs=2000)
    print(f"l2={l2:.0e} -> MSE_va={m:.6f}")


grid_lr = [1e-4, 2e-4, 3e-4, 5e-4, 7e-4, 1e-3]
res = []
for lr in grid_lr:
    m = train_mlp(Xtr_c, ytr, Xva_c, yva, lr=lr, l2=1e-5, d_h1=32, d_h2=16, epochs=2000, seed=42)
    res.append((lr, m))
    print(f"lr={lr:.0e} -> MSE_va={m:.6f}")

res_sorted = sorted(res, key=lambda t: t[1])
print("Top-2:", res_sorted[:2])


grid_l2 = [0.0, 1e-6, 1e-5, 5e-5, 1e-4]
for l2 in grid_l2:
    m = train_mlp(Xtr_c, ytr, Xva_c, yva,
                  lr=1e-3, l2=l2, d_h1=32, d_h2=16,
                  epochs=2000, seed=42)
    print(f"l2={l2:.0e} -> MSE_va={m:.6f}")


# experimenta con diferentes tamaños para las capas ocultas (d_h1 y d_h2) para ver cómo afectan el rendimiento del modelo.
for h in [8, 16, 32, 64]:
    m = train_mlp(Xtr_c, ytr, Xva_c, yva,
                  lr=1e-3, l2=0, d_h1=h, d_h2=h//2,
                  epochs=2000, seed=42)
    print(f"d_h1={h}, d_h2={h//2} -> MSE_va={m:.6f}")


# Nueva función predict
# define una nueva función predict que utiliza los pesos y sesgos ya entrenados para realizar predicciones en nuevos datos de entrada.
def predict(Xnew):
    return relu(relu(Xnew@W1+b1)@W2+b2)@W3+b3


#  Forward final
'''
utiliza la función predict para obtener las predicciones en los conjuntos de entrenamiento y validación.
Luego, calcula y muestra el Error Absoluto Medio (MAE) para ambos conjuntos.
Finalmente, genera tres gráficos para visualizar el rendimiento del modelo:
'''
ytr_hat = predict(Xtr).reshape(-1)
yva_hat = predict(Xva).reshape(-1)
ytr_1d = ytr.reshape(-1)
yva_1d = yva.reshape(-1)

print("MAE_tr:", mae(ytr_hat, ytr_1d), "MAE_va:", mae(yva_hat, yva_1d))

# 1) y vs y_hat
plt.scatter(yva_1d, yva_hat, s=25)
plt.xlabel("y real"); plt.ylabel("y predicha")
plt.title("Predicción vs Real (Validación)")
plt.plot([0,1],[0,1],'r')
plt.show()

# 2) residuos
res = yva_1d - yva_hat
plt.scatter(yva_hat, res, s=25)
plt.xlabel("y_pred"); plt.ylabel("residuo")
plt.title("Residuos vs y_pred")
plt.axhline(0, color='r'); plt.show()

# 3) histograma
plt.hist(res, bins=20); plt.title("Histograma de residuos"); plt.show()



# métricas
'''
calcula y muestra métricas adicionales para evaluar el rendimiento del modelo en el conjunto de validación,
incluyendo el Error Absoluto Medio (MAE) y el coeficiente de determinación R2.
También genera un gráfico de dispersión de los residuos vs. los valores predichos en el conjunto de validación.
'''

from sklearn.metrics import r2_score
yva_hat = predict(Xva_c)
print("MAE_va:", np.mean(np.abs(yva_hat - yva)))
print("R2_va:", r2_score(yva, yva_hat))

# residuos
res = (yva - yva_hat).reshape(-1)
plt.scatter(yva_hat.reshape(-1), res, s=25); plt.axhline(0, color='r'); plt.show()


from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
'''
entrena un modelo de regresión lineal utilizando los mismos datos centrados que se usaron para entrenar la red neuronal.
Esto se hace para comparar el rendimiento de la red neuronal con un modelo lineal más simple.
Luego, calcula y muestra las métricas de evaluación (MSE, MAE, R2) para el modelo de regresión lineal en los conjuntos de entrenamiento y validación.
'''

# Entrenar modelo lineal con los mismos datos centrados
lin = LinearRegression()
lin.fit(Xtr_c, ytr)

# Predicción y métricas
yva_hat_lin = lin.predict(Xva_c)
ytr_hat_lin = lin.predict(Xtr_c)

print("Regresión Lineal:")
print(" MSE_tr:", mean_squared_error(ytr, ytr_hat_lin))
print(" MSE_va:", mean_squared_error(yva, yva_hat_lin))
print(" MAE_va:", mean_absolute_error(yva, yva_hat_lin))
print(" R2_va:", r2_score(yva, yva_hat_lin))


'''
 muestra los coeficientes y el intercepto del modelo de regresión lineal entrenado en la celda anterior.
 Esto permite ver la relación lineal que el modelo ha encontrado entre las características y la variable objetivo.
'''
print("Coeficientes:", lin.coef_)
print("Intercepto:", lin.intercept_)


'''
define una nueva función llamada train_mlp_track. Esta función es similar a train_mlp,
pero con la diferencia de que registra las pérdidas (MSE) en los conjuntos de entrenamiento y validación cada 50 épocas.
Esto es útil para visualizar las curvas de aprendizaje.
'''
def train_mlp_track(Xtr_c, ytr, Xva_c, yva, *,
                    d_h1=32, d_h2=16, lr=1e-3, l2=0,
                    epochs=2000, batch=32, seed=42):
    rng = np.random.default_rng(seed)
    d_in, d_out = Xtr_c.shape[1], 1
    W1 = rng.normal(0, np.sqrt(2/d_in),  (d_in,d_h1)); b1 = np.zeros(d_h1)
    W2 = rng.normal(0, np.sqrt(2/d_h1), (d_h1,d_h2));  b2 = np.zeros(d_h2)
    W3 = rng.normal(0, np.sqrt(2/d_h2), (d_h2,d_out)); b3 = np.zeros(d_out)

    def relu(z): return np.maximum(0,z)
    def mse(yh,y): return np.mean((yh-y)**2)
    N = len(Xtr_c)

    hist_tr, hist_va = [], []

    for ep in range(epochs):
        for s in range(0, N, batch):
            xb, yb = Xtr_c[s:s+batch], ytr[s:s+batch]

            z1 = xb @ W1 + b1; h1 = relu(z1)
            z2 = h1 @ W2 + b2; h2 = relu(z2)
            yhat = h2 @ W3 + b3

            dY  = (yhat - yb)/len(xb)
            gW3 = h2.T @ dY + (2*l2/N)*W3; gb3 = dY.sum(0)
            dh2 = dY @ W3.T; dz2 = dh2 * (z2>0)
            gW2 = h1.T @ dz2 + (2*l2/N)*W2; gb2 = dz2.sum(0)
            dh1 = dz2 @ W2.T; dz1 = dh1 * (z1>0)
            gW1 = xb.T @ dz1 + (2*l2/N)*W1; gb1 = dz1.sum(0)

            W3 -= lr*gW3; b3 -= lr*gb3
            W2 -= lr*gW2; b2 -= lr*gb2
            W1 -= lr*gW1; b1 -= lr*gb1

        # registrar pérdida cada 50 épocas
        if (ep+1) % 50 == 0:
            def fwd(Xc): return relu(relu(Xc@W1+b1)@W2+b2)@W3+b3
            hist_tr.append(mse(fwd(Xtr_c), ytr))
            hist_va.append(mse(fwd(Xva_c), yva))

    return hist_tr, hist_va



#para graficar las curvas de aprendizaje (pérdida vs. épocas) para los conjuntos de entrenamiento y validación.
#Estas curvas ayudan a diagnosticar si el modelo está sobreajustando o subajustando.
hist_tr, hist_va = train_mlp_track(Xtr_c, ytr, Xva_c, yva)

plt.plot(range(50,2001,50), hist_tr, label="train")
plt.plot(range(50,2001,50), hist_va, label="val")
plt.xlabel("épocas"); plt.ylabel("MSE")
plt.title("Curvas de aprendizaje")
plt.legend(); plt.show()


# Cambiar tasa de aprendizaje
#hist_tr, hist_va = train_mlp_track(Xtr_c,ytr,Xva_c,yva, lr=7e-4)

# Cambiar capacidad
#hist_tr, hist_va = train_mlp_track(Xtr_c,ytr,Xva_c,yva, d_h1=48, d_h2=24)

# Regularización y batch
#hist_tr, hist_va = train_mlp_track(Xtr_c,ytr,Xva_c,yva, l2=1e-5, batch=64)

# Más épocas y semilla fija para reproducibilidad
#hist_tr, hist_va = train_mlp_track(Xtr_c,ytr,Xva_c,yva, epochs=3000, seed=7)



