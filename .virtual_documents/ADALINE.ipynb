import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


df = pd.read_csv("train.csv")


df_val = pd.read_csv("test.csv")


df.describe()


df_val.describe()


# Semilla para reproducibilidad
rng = np.random.default_rng(7)

X = df[["x1","x2","x3","x4"]].to_numpy(dtype=float)
y = df["d"].to_numpy(dtype=float)
N, d = X.shape

rng = np.random.default_rng(7)
idx = rng.permutation(len(X))
n_tr = int(0.8*len(X))
tr, va = idx[:n_tr], idx[n_tr:]
X_tr, y_tr = X[tr], y[tr]
X_va, y_va = X[va], y[va]

#Estandarización (z-score) con stats de TRAIN
mu = X_tr.mean(axis=0)
sigma = X_tr.std(axis=0, ddof=0)
sigma[sigma == 0] = 1.0  # evita división por cero

X_tr_std = (X_tr - mu)/sigma
X_va_std = (X_va - mu)/sigma
X2_std = (df_val[["x1","x2","x3","x4"]].to_numpy(float) - mu)/sigma
print("X_std shape:", X_tr_std.shape, "y shape:", y.shape)
print("medias:", mu, "desv.std:", sigma)


#algoritmo de prueba 1
class AdalineGD:
    def __init__(self, eta=0.01, n_epochs=50, random_state=7):
        self.eta = eta
        self.n_epochs = n_epochs
        self.rng = np.random.default_rng(random_state)
        # Se inicializan al entrenar: self.w (d,), self.w0 (esc), self.J_history (list)

    def net_input(self, X):
        # z = X @ w + w0
        return X @ self.w + self.w0

    def fit(self, X, y):
        N, d = X.shape
        # inicialización pequeña
        self.w = self.rng.uniform(0.0, 1.0, size=d) # 1 se inicializan los pesos w aleatorio entre 0 y 1
        self.w0 = 0.0 # el bias 
        self.J_history = []

        for _ in range(self.n_epochs):#se realiza el proceso de entrenamiento de acuerdo a las epocas y demas parametros 
            z = self.net_input(X)            # (N,)
            y_hat = z                        # salida lineal
            errors = y - y_hat               # (N,)

            # Gradientes de J = 1/2 sum (y - y_hat)^2
            # dJ/dw = - X^T @ errors
            # dJ/dw0 = - sum(errors)
            grad_w  = -(X.T @ errors)
            grad_w0 = -errors.sum()

            # Actualización
            self.w  -= self.eta * grad_w
            self.w0 -= self.eta * grad_w0

            # Coste MSE/2 por época (opcional: promedio)
            J = 0.5 * np.mean(errors**2)
            self.J_history.append(J)
        return self

    def predict_continuous(self, X):
        return self.net_input(X)

    def predict_class(self, X):
        return np.where(self.net_input(X) >= 0.0, 1, -1)

# ---- Entrenamiento en X_std, y (del paso previo) ----
adaline = AdalineGD(eta=0.0025, n_epochs=50, random_state=7).fit(X_tr_std, y_tr)

print("w:", adaline.w)
print("w0:", adaline.w0)
print("J[0], J[-1]:", adaline.J_history[0], adaline.J_history[-1])

y_pred = adaline.predict_class(X_tr_std)
acc = (y_pred == y_tr).mean()
print("accuracy:", acc)


# --- curva de coste ---
plt.figure()
plt.plot(adaline.J_history)
plt.xlabel("Época")
plt.ylabel("J = 0.5·MSE")
plt.title("Adaline - Curva de coste")
plt.show()

# --- frontera de decisión en 2D ---
def plot_decision_boundary(model, X, y):
    # Usar solo las dos primeras dimensiones
    X_2d = X[:, :2]  # Seleccionar solo x1 y x2
    
    x1_min, x1_max = X_2d[:,0].min()-0.5, X_2d[:,0].max()+0.5
    x2_min, x2_max = X_2d[:,1].min()-0.5, X_2d[:,1].max()+0.5
    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 300),
                          np.linspace(x2_min, x2_max, 300))
    
    # Crear grid 2D y rellenar con ceros las otras dimensiones
    grid = np.zeros((xx1.ravel().shape[0], X.shape[1]))
    grid[:, 0] = xx1.ravel()
    grid[:, 1] = xx2.ravel()
    
    Z = model.predict_class(grid).reshape(xx1.shape)

    plt.figure()
    plt.contourf(xx1, xx2, Z, alpha=0.25)
    plt.scatter(X_2d[y==1,0], X_2d[y==1,1], marker='o', label='+1')
    plt.scatter(X_2d[y==-1,0], X_2d[y==-1,1], marker='s', label='-1')
    
    plt.legend()
    plt.title("Adaline - Frontera de decisión (primeras 2 dimensiones)")
    plt.xlabel("x1 (std)")
    plt.ylabel("x2 (std)")
    plt.show()

plot_decision_boundary(adaline, X_tr_std, y_tr)


class AdalineGDTrack:
    def __init__(self, eta=0.01, n_epochs=10, random_state=7):
        self.eta = eta
        self.n_epochs = n_epochs
        self.rng = np.random.default_rng(random_state)

    def net_input(self, X):
        return X @ self.w + self.b

    def fit(self, X, y):
        N, d = X.shape
        self.w = self.rng.uniform(0.0, 1.0, size=d)# 1 se inicializan los pesos w aleatorio entre 0 y 1
        self.b = 0.0
        self.J_history = []      # 0.5 * MSE por época
        self.mse_history = []    # MSE por época
        logs = []
        for epoch in range(1, self.n_epochs+1):#se realiza el proceso de entrenamiento de acuerdo a las epocas y demas parametros 
            z = self.net_input(X)
            err = y - z
            grad_w  = -(X.T @ err) / N
            grad_b  = -err.mean()

            # guardar estado inicial de la época
            w_init = self.w.copy()
            b_init = self.b

            # actualización
            self.w -= self.eta * grad_w
            self.b -= self.eta * grad_b

            # estado final
            w_final = self.w.copy()
            b_final = self.b

            J = 0.5 * np.mean(err**2)

            mse = np.mean(err**2)     # dentro del fit(), por época
            self.mse_history.append(mse)            
            self.J_history.append(J)
            # fila para el dataframe (init + final en la misma fila)
            row = {"epoch": epoch, "J": J, "b_init": b_init, "b_final": b_final}
            for j in range(d):
                row[f"w{j}_init"]  = w_init[j]
                row[f"w{j}_final"] = w_final[j]
                # opcional: deltas
                row[f"dw{j}"] = w_final[j] - w_init[j]
            row["db"] = b_final - b_init
            logs.append(row)

        self.history_df = pd.DataFrame(logs)
        return self

    def predict_class(self, X):
        return np.where(self.net_input(X) >= 0, 1, -1)


# Ejemplo de uso (asumiendo X_std, y creados antes):
adal = AdalineGDTrack(eta=0.0025, n_epochs=800).fit(X_tr_std, y_tr)# el numero de iteraciones (epocas) afecta terriblemente el error cuadratico medio, explicar 
df = adal.history_df
df.head()# los d corresponde a los deltas obtenidos del gradiente 


plt.plot(adal.mse_history); plt.xlabel("Época"); plt.ylabel("MSE"); plt.show()








def adaline_run_summary(X, y, n_epochs=20, eta=0.01, seed=7):
    """
    Devuelve una fila con w_init (época 1) y w_final (última época).
    Solo pesos w, sin bias.
    """
    rng = np.random.default_rng(seed)
    N, d = X.shape
    w = rng.uniform(0.0, 1.0, size=d)  # w ~ U[0,1)# 1 se inicializan los pesos w aleatorio entre 0 y 1
    b = 0.0

    w_init_first = w.copy()  # vector inicial de la 1ª época

    for _ in range(n_epochs):#se realiza el proceso de entrenamiento de acuerdo a las epocas y demas parametros 
        z = X @ w + b
        err = y - z
        grad_w = -(X.T @ err) / N
        grad_b = -err.mean()
        w -= eta * grad_w
        b -= eta * grad_b

    w_final_last = w.copy()  # vector final de la última época

    row = {"n_epochs": n_epochs}
    for j in range(d):
        row[f"w{j}_init"]  = w_init_first[j]
        row[f"w{j}_final"] = w_final_last[j]
    return row




# ----- 5 llamadas con distintos epochs -----
epochs_list = [20, 40, 80, 160, 320]
eta = 0.0025
seed_base = 7

rows = []
for i, ne in enumerate(epochs_list, start=1):
    row = adaline_run_summary(X_tr_std, y_tr, n_epochs=ne, eta=eta, seed=seed_base + i)
    row["call"] = f"call_{i}"
    rows.append(row)

df_summary_calls = pd.DataFrame(rows, columns=["call","n_epochs"] +
                                sum([[f"w{j}_init", f"w{j}_final"] for j in range(X_tr_std.shape[1])], []))

df_summary_calls.head()
# df_summary_calls.to_csv("adaline_resumen_w_inicial_final_por_llamada.csv", index=False)


def adaline_run_mse(X, y, n_epochs=20, eta=0.01, seed=7):
    rng = np.random.default_rng(seed)
    N, d = X.shape
    w = rng.uniform(0.0, 1.0, size=d)# 1 se inicializan los pesos w aleatorio entre 0 y 1
    b = 0.0
    mse_hist = []

    for _ in range(n_epochs):#se realiza el proceso de entrenamiento de acuerdo a las epocas y demas parametros 
        z = X @ w + b
        err = y - z
        mse_hist.append(np.mean(err**2))     # MSE de esta época (antes de actualizar)
        grad_w = -(X.T @ err) / N
        grad_b = -err.mean()
        w -= eta * grad_w
        b -= eta * grad_b

    return mse_hist

#entrenamientos con distintos números de épocas ---
epochs_list = [20, 40, 80, 160, 320]
eta = 0.01
seed_base = 7

histories = []
for i, ne in enumerate(epochs_list, start=1):
    mse_hist = adaline_run_mse(X_tr_std, y_tr, n_epochs=ne, eta=eta, seed=seed_base + i)
    histories.append((f"run{i} ({ne} épocas)", mse_hist))

# --- un solo plot con todas las curvas ---
plt.figure()
for label, hist in histories:
    plt.plot(range(1, len(hist)+1), hist, label=label)
plt.xlabel("Época")
plt.ylabel("MSE")
plt.title("Adaline: MSE por entrenamiento")
plt.legend()
plt.show()


# --- 3) Adaline desde cero ---
class AdalineGD:
    def __init__(self, eta=0.01, n_epochs=50, seed=7):
        self.eta=eta; self.n_epochs=n_epochs
        self.rng = np.random.default_rng(seed)
    def fit(self, X, y):
        N,d = X.shape
        self.w = self.rng.uniform(0.0,1.0,size=d)  # init U[0,1) # 1 se inicializan los pesos w aleatorio entre 0 y 1
        self.b = 0.0
        for _ in range(self.n_epochs):#se realiza el proceso de entrenamiento de acuerdo a las epocas y demas parametros 
            z = X @ self.w + self.b
            err = y - z
            grad_w = -(X.T @ err)/N
            grad_b = -err.mean()
            self.w -= self.eta*grad_w
            self.b -= self.eta*grad_b
        return self
    def predict_cont(self, X):  # score lineal
        return X @ self.w + self.b
    def predict_class(self, X):
        return np.where(self.predict_cont(X)>=0, 1.0, -1.0)

# Entrenamientos con distintas épocas y clasificación del dataset de validacion 
epochs_list = [20, 40, 80, 160, 320]
eta = 0.0025
preds = pd.DataFrame(index=df_val.index)   # almacenará predicciones por run
scores = pd.DataFrame(index=df_val.index)  # almacenará scores lineales

for i, ne in enumerate(epochs_list, start=1):
    model = AdalineGD(eta=eta, n_epochs=ne, seed=7+i).fit(X_tr_std, y_tr)
    y2_score = model.predict_cont(X2_std)
    y2_pred  = np.where(y2_score>=0, 1.0, -1.0)
    preds[f"pred_run{i}_e{ne}"]  = y2_pred
    scores[f"score_run{i}_e{ne}"] = y2_score

# voto mayoritario
preds["vote_majority"] = np.sign(preds.filter(like="pred_run").sum(axis=1)).replace(0, 1.0)
# promedio de scores -> signo
scores["avg_score"] = scores.mean(axis=1)
preds["sign_avg_score"] = np.where(scores["avg_score"]>=0, 1.0, -1.0)

resultado = pd.concat([df_val.reset_index(drop=True), preds.reset_index(drop=True), scores[["avg_score"]].reset_index(drop=True)], axis=1)
resultado.head(20)
# resultado.to_csv("clasificacion_dataset2_varias_epocas.csv", index=False)












